install.packages("ElemStatLearn")
library(ElemStatLearn)
data(ozone, package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
ll <- matrix(NA, nrow=10, ncol=155)
ll <- matrix(NA, nrow=10, ncol=155)
for(i in 1:10){
#resample 10 different times over the whole dataset with replacement
ss <- sample(1:dim(ozone)[1],replace=T)
#create subset off the random sample above, then reorder the dataset by the ozone variable
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
#fit a loess curve (smooth curve fit through the data - similar to spline model fits), relating temperature to the ozone varaible on our subsampled data, span=0.2 is a measure of how smooth that fit will be
loess0 <- loess(temperature ~ ozone, data=ozone0, span=0.2)
#predict for every loess curve the outcome of a new dataset for the exact same values. The ith row of the ll object is now the prediction from the loess curve from the ith resampled data from the ozone
ll[i,] <- predict(loess0, newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10){lines(1:155,ll[1,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean), col="red", lwd=2)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean), col="red", lwd=2)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B=10,
bagControl = bagControl(fit=ctreeBag$fit,
predict= ctreeBag$pred,
aggregate = ctreeBag$aggregate))
library(caret)
treebag <- bag(predictors, temperature, B=10,
bagControl = bagControl(fit=ctreeBag$fit,
predict= ctreeBag$pred,
aggregate = ctreeBag$aggregate))
install.packages("party")
treebag <- bag(predictors, temperature, B=10,
bagControl = bagControl(fit=ctreeBag$fit,
predict= ctreeBag$pred,
aggregate = ctreeBag$aggregate))
plot(ozone$ozone, temperature, col="lightgrey", pch=19)
points(ozone$ozone, predict(treebag$fits[[1]]$fit, predictors), pch=19,col="red")
points(ozone$ozone, predict(treebag, predictors), pch=19, col="blue")
ctreeBag$fit
ctreeBag$pred
ctreeBag$aggregate
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <-iris[-inTrain,]
dim(training);dim(testing)
modFit <- train(Species ~. , data=training, method="rf", prox=TRUE)
modFit <- train(Species ~. , data=training, method="rf", prox=TRUE)
modFit
getTree(modFit$finalModel, k=2)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP)
irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species, data=training)
source('~/.active-rstudio-document', echo=TRUE)
p + geom_point(aes(x=Petal.Width, y=Petal.Length, col=Species), size=5, shape=4, data=irisP)
pred <- predict(modFit, testing)
testing$predRight <- pred == testing$Species
table(pred, testing$Species)
qplot(Petal.Width, Petal.Length, colour=predRight, data=testing, main="newdata Predictions")
library(ISLR); data(Wage); library(ggplot2); library(caret)
Wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- wage[-inTrain,]
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
modFit <- train(wage ~. , method="glm", data=training, verbose=FALSE)
print(modFit)
modFit <- train(wage ~. , method="glm", data=training, verbose=FALSE)
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
print(modFit)
qplot(predict(modFit, testing), wage, data=testing)
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(testing); dim(training)
modlda <- train(Species ~. , data=training, method= "lda")
modlda <- train(Species ~. , data=training, method= "lda")
modnb <- train(Species ~., data=training, method="nb")
modnb <- train(Species ~., data=training, method="nb")
plda = predict(modlda, testing)
pnb = predict(modnb, testing)
table(plda, pnb)
equalPredictions = (plda==pnb)
qplot(Petal.Width, Sepal.Width, colour=equalPredictions,data=testing)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
inTrain <- createDataPartition(y=AppliedPredictiveModeling$Case,
p=0.7, list=FALSE)
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
dim(training)
dim(testing)
set.seed(125)
modFit <- train(Case ~. , method="rpart", data=training)
modFit <- train(Case ~. , method="rpart", data=training)
print(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
library(caret)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list= FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training)
dim(testing)
#plot petal width vs the sepal width
qplot(Petal.Width, Sepal.Width, colour=Species, data=training) #three very distinct clustering. Maybe challenging for linear model, but not for classification trees.
#rpart = r's package to do regression trees
modFit <- train(Species ~. , method="rpart", data=training)
print(modFit$finalModel)
#ex: Petal.Length < 2.6 and if that happens, then all [1.00, 0.00, 0.00] those Petal.Lengths belong to the species "setosa"
#plot classification tree (dendrogram)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
#easier/prettier plot
library(rattle)
fancyRpartPlot(modFit$finalModel)
#predict new values based on a class label
predict(modFit, newdata=testing)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
dim(training)
dim(testing)
set.seed(125)
#fit a CART model with the rpart method using all predictor variables and default caret settings
#rpart = r's package to do regression trees
modFit <- train(Case ~. , method="rpart", data=training)
print(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
dim(training)
dim(testing)
set.seed(125)
modFit <- train(Case ~. , method="rpart", data=training)
library(rattle)
fancyRpartPlot(modFit$finalModel)
predict(modFit, newdata=testing)
names(segmentationOriginal)
segmentationOriginal$Class
inTrain <- createDataPartition(y=segmentationOriginal$Class,
p=0.7, list=FALSE)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
segmentationOriginal$Class
inTrain <- createDataPartition(y=segmentationOriginal$Class,
p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
dim(training)
dim(testing)
set.seed(125)
modFit <- train(Class ~. , method="rpart", data=training)
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(pgmm)
data(olive)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
names(olive)
newdata = as.data.frame(t(colMeans(olive)))
modFit <- train(Area ~. , method="rpart", data=olive)
library(rattle)
fancyRpartPlot(modFit$finalModel)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit, newdata=newdata)
olive$Area
predict(modFit, newdata=newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(trainSA);dim(testSA)
set.seed(13234)
names(SAheart)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=olive, family="binomial")
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass
missClass(modFit$response)
missClass(modFit$trainSA, response)
missClass(modFit$trainSA, "response")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(trainSA);dim(testSA)
set.seed(13234)
#fit a logistic regression model (method="glm", be sure to specify family="binomial") with Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol consumption, obesity levels, cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors.
names(SAheart)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
#Calculate the misclassification rate for your model using this function and a prediction on the "response" scale:
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
missClass(modFit$trainSA, "response")
print(modFit)
missClass(modFit, "response")
missClass(modFit$chd, "response", data=trainSA)
missClass(modFit$trainSA, "response")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(trainSA);dim(testSA)
set.seed(13234)
#fit a logistic regression model (method="glm", be sure to specify family="binomial") with Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol consumption, obesity levels, cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors.
names(SAheart)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
#Calculate the misclassification rate for your model using this function and a prediction on the "response" scale:
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred <- predict(modFit, testSA)
testing$predRight <- pred == testing$chd #set a variable to see if we got the prediction right
testing$predRight <- pred == testSA$chd #set a variable to see if we got the prediction right
testSA$predRight <- pred == testSA$chd #set a variable to see if we got the prediction right
table(pred, testSA$chd) #see that we missed 2 overall
missClass(testSA$predRight, "response")
missClass(pred$trainSA, "response")
missClass(trainSA$pred, "response")
predtest <- predict(modFit, testSA)
predtrain <- predict(modFit, trainSA)
missClass(predtest, "response")
missClass(predtrain, "response")
missClass(trainSA$chd, "response")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(trainSA);dim(testSA)
set.seed(13234)
#fit a logistic regression model (method="glm", be sure to specify family="binomial") with Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol consumption, obesity levels, cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors.
names(SAheart)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
#Calculate the misclassification rate for your model using this function and a prediction on the "response" scale:
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
missClass(trainSA$chd, "response")
missClass(testSA$chd, "response")
missClass(modFit$chd, "response")
predtrain <- predict(modFit$chd, newdata=trainSA)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(trainSA);dim(testSA)
names(SAheart)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
#Calculate the misclassification rate for your model using this function and a prediction on the "response" scale:
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
missClass(trainSA$chd, "response")
missClass(testSA$chd, "response")
names(modFit)
missClass(trainSA$chd, modeFit$pred)
missClass(trainSA$chd, modFit$pred)
missClass(trainSA$chd, "response")
missClass(testSA$chd, "response")
1 - missClass(trainSA$chd, "response")
1 - missClass(testSA$chd, "response")
modFit <- train(chd ~ ., method="glm", data=trainSA, family="binomial")
missClass(trainSA$chd, "response")
missClass(testSA$chd, "response")
set.seed(13234)
#fit a logistic regression model (method="glm", be sure to specify family="binomial") with Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol consumption, obesity levels, cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors.
names(SAheart)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
pred = predict(modFit, testSA) #lda model predicting on test set
predtrain = predict(modFit, trainSA) #nb model predicting on test set
table(pred, predtrain)
equalPredictions = (pred==predtrain)
missClass(trainSA$equalPredictions, "response")
missClass(predtrain$chd, "response")
missClass(predtrain, "response")
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
names(vowel.train)
names(vowel.test)
names(vowel.train); names(vowel.test)
y.f <- factor(vowel.train$y)
y.ftrain <- factor(vowel.train$y)
y.ftrain <- factor(vowel.train$y)
y.ftest <- factor(vowel.test$y)
y.ftrain
vowel.train$y.ftrain <- factor(vowel.train$y)
vowel.train$y.f <- factor(vowel.train$y)
vowel.test$y.f <- factor(vowel.test$y)
names(vowel.train); names(vowel.test)
vowel.train$y.f <- factor(vowel.train$y)
vowel.test$y.f <- factor(vowel.test$y)
set.seed(33833)
vowel.train$yf <- factor(vowel.train$y)
vowel.test$yf <- factor(vowel.test$y)
set.seed(33833)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
names(vowel.train); names(vowel.test)
vowel.train$yf <- factor(vowel.train$y)
vowel.test$yf <- factor(vowel.test$y)
set.seed(33833)
modFit <- train(yf ~. , data=vowel.train, method="rf", prox=TRUE) #prox=TRUE because it produces extra info...?
modFit <- train(yf ~. , data=vowel.train, method="rf", prox=TRUE) #prox=TRUE because it produces extra info...?
training <- vowel.train[-1,]
training <- vowel.train[-1,]; names(training)
training <- vowel.train[,-1]; names(training)
testing <- vowel.test[,-1]; names(testing)
#Q5 ####
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
#Set the variable y to be a factor variable in both the training and test set.
names(vowel.train); names(vowel.test)
vowel.train$yf <- factor(vowel.train$y)
vowel.test$yf <- factor(vowel.test$y)
training <- vowel.train[,-1]; names(training)
testing <- vowel.test[,-1]; names(testing)
#Then set the seed to 33833.
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
modFit <- train(yf ~. , data=vowel.train, method="rf", prox=TRUE)
modFit
library(caret)
varImp(modFit)
#Q5 ####
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
#Set the variable y to be a factor variable in both the training and test set.
names(vowel.train); names(vowel.test)
vowel.train$yf <- factor(vowel.train$y)
vowel.test$yf <- factor(vowel.test$y)
training <- vowel.train[,-1]; names(training)
testing <- vowel.test[,-1]; names(testing)
#Then set the seed to 33833.
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
modFit <- train(yf ~. , data=vowel.train, method="rf", prox=TRUE)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
#Set the variable y to be a factor variable in both the training and test set.
names(vowel.train); names(vowel.test)
vowel.train$yf <- factor(vowel.train$y)
vowel.test$yf <- factor(vowel.test$y)
training <- vowel.train[,-1]; names(training)
testing <- vowel.test[,-1]; names(testing)
#Then set the seed to 33833.
set.seed(33833)
#Then set the seed to 33833.
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
modFit <- train(yf ~. , data=training, method="rf", prox=TRUE)
modFit
#The caret package uses by defualt the Gini importance.
#Calculate the variable importance using the varImp function in the caret package.
library(caret)
varImp(modFit)
getTree(modFit$newdata)
getTree(modFit$finalModel)
library(pgmm)
data(olive)
olive = olive[,-1]
#These data contain information on 572 different Italian olive oils from multiple regions in Italy.
#Fit a classification tree where Area is the outcome variable.
names(olive)
modFit <- train(Area ~. , method="rpart", data=olive)
#easier/prettier plot
library(rattle)
fancyRpartPlot(modFit$finalModel)
#Then predict the value of area for the following data frame using the tree command with all defaults
newdata = as.data.frame(t(colMeans(olive)))
#predict new values based on a class label
predict(modFit, newdata=newdata)
getTree(modFit$finalModel)
modFit2 <- train(Area ~. , data=olive, method="rf", prox=TRUE)
getTree(modFit2$finalModel)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(trainSA);dim(testSA)
set.seed(13234)
#fit a logistic regression model (method="glm", be sure to specify family="binomial") with Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol consumption, obesity levels, cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors.
names(SAheart)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
#Calculate the misclassification rate for your model using this function and a prediction on the "response" scale:
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
names(trainSA)
names(SAheart)
names(missClass)
names(modFit)
missClass(trainSA$chd, trainSA$chd)
missClass(testSA$chd, testSA$chd)
pred = predict(modFit, trainSA)
names(pred)
missClass(trainSA$chd, pred)
missClass(testSA$chd, pred)
#Q4 ####
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(trainSA);dim(testSA)
set.seed(13234)
#fit a logistic regression model (method="glm", be sure to specify family="binomial") with Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol consumption, obesity levels, cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors.
names(SAheart)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
#Calculate the misclassification rate for your model using this function and a prediction on the "response" scale:
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred = predict(modFit, trainSA)
missClass(trainSA$chd, pred)
missClass(testSA$chd, pred)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
segmentationOriginal$Class
names(segmentationOriginal)
segmentationOriginal$Case
training <- segmentationOriginal[which(segmentationOriginal$Case=="Train"),]
testing <- segmentationOriginal[which(segmentationOriginal$Case=="Test"),]
dim(training)
dim(testing)
dim(segmentationOriginal)
set.seed(125)
modFit <- train(Class ~. , method="rpart", data=training)
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(trainSA);dim(testSA)
set.seed(13234)
names(SAheart)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred = predict(modFit, testSA)
missClass(trainSA$chd, pred)
missClass(testSA$chd, pred)
predtrain = predict(modFit, trainSA)
predtest = predict(modFit, testSA)
missClass(trainSA$chd, predtrain)
missClass(testSA$chd, predtest)
setwd("C:/Users/Katie/Desktop/MachineLearning/Project/ML_project")
wd()
wd
pwd
ls
